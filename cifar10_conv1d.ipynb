{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we present an algorithm that maps 2D data to 1D and show that 1D CNNs comparable with 2D CNNs. In particular, we read image pixels with Hilbert and modified Sierpinski space-filling curves, and then apply one-dimensional convolutional neural networks. \n",
    "\n",
    "We know that space-filling curves preserve a maximum of spatial locality information between elements. We believe that using space-filling curve mapping as a preprocessing tool, preserves enough information of neighbouring pixels to be used by 1D CNNs. We evaluated the classification performance of 1D CNNs on Cifar-10 image dataset. This mapping technique reduces CNN training cost while performs comparably to 2D CNNs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras import regularizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "\n",
    "from space_filling_curves import rix, hilbert_curve, scan_image_with_hilbert, modified_sierpinski_curve, scan_image_with_sierpinski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of 60000 32x32x3 color images in 10 equal classes. There are 50000 training images and 10000 test images. Each class of images corresponds to a physical object such as airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n",
      "It is a bird!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "num_classes = 10\n",
    "cifar10_classes = {0:'airplane', 1:'automobile', 2:'bird', 3:'cat', 4:'deer', \n",
    "                      5:'dog', 6:'frog', 7:'horse', 8:'ship', 9:'truck'}\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "def draw_cifar_image(i):\n",
    "    img = x_train[i]\n",
    "    label = y_train[i]    \n",
    "    plt.imshow(img)\n",
    "\n",
    "    \n",
    "print(\"It is a \" + str(cifar10_classes[int(y_train[13])]) + \"!\")\n",
    "draw_cifar_image(13)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the image pixels with Hilbert space-filling curve or modified Sierpinski space-filling curve. The main difference between these two curves is that modified Sierpinski curve is more symmetrical and closed.\n",
    "\n",
    "Here, for example, we read the red matrix of the 4x4x3 image with Hilbert curve in the following order: 35, 19, 22, 13, 4, 0, 8, 3, 7, 1, 3, 10, 53, 16, 25, 6. We read the red matrix of the 4x4x3 image with modified Sierpinski curve in the following order: 35, 19, 22, 25, 6, 53, 16, 10, 3, 1, 7, 8, 0, 4, 3, 13. The code in Python 3 (see space_filling_curves.py) shows how to build the modified Nth order Sierpinski curve and Nth order Hilbert curve.\n",
    "\n",
    "\n",
    "2nd-order Hilbert curve | 2nd-order modified Sierpiński curve\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"images/hilbertRGB.png\" style=\"width:400px;height:300px\"/>  |  <img src=\"images/readwithcurve.png\" style=\"width:400px;height:300px\"/>\n",
    "\n",
    "\n",
    "\n",
    "Although it is possible to use lower order of the curves to read images, it is not recommended for small images. Here we preserve every pixel in image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The order of the curve is 5\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n",
      "(50000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "num_px = x_train.shape[1]\n",
    "N = math.log10(num_px) / math.log10(2)\n",
    "N = int(N) # to cover all pixels\n",
    "print(\"The order of the curve is \" + str(N)) \n",
    "#x,y = modified_sierpinski_curve(N)\n",
    "x,y = hilbert_curve(N)\n",
    "\n",
    "# normalize data\n",
    "X_train_orig = x_train/255.\n",
    "X_test_orig = x_test/255.\n",
    "mtrain = X_train_orig.shape[0]\n",
    "mtest = X_test_orig.shape[0]\n",
    "\n",
    "# convert to one-hot presentation\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# print shapes\n",
    "print(X_train_orig.shape)\n",
    "print(X_test_orig.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create arrays of zeros and read pixels with Hilbert curve for Red, Green, and Blue channels. The data therefore transformed from (number of examples, number of pixels, number of pixels, 3) to (number of examples, $2^{2N}$, 3), where $N$ is the order of the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1024, 3)\n",
      "(10000, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.zeros([X_train_orig.shape[0], 2**(2*N), X_train_orig.shape[3]])\n",
    "test_data = np.zeros([X_test_orig.shape[0], 2**(2*N), X_test_orig.shape[3]])\n",
    "\n",
    "train_data[:, :, 0] = scan_image_with_hilbert(N, mtrain, X_train_orig[:, :, :, 0], x, y)        \n",
    "test_data[:, :, 0] = scan_image_with_hilbert(N, mtest, X_test_orig[:, :, :, 0], x, y) \n",
    "\n",
    "train_data[:, :, 1] = scan_image_with_hilbert(N, mtrain, X_train_orig[:, :, :, 1], x, y)        \n",
    "test_data[:, :, 1] = scan_image_with_hilbert(N, mtest, X_test_orig[:, :, :, 1], x, y) \n",
    "\n",
    "train_data[:, :, 2] = scan_image_with_hilbert(N, mtrain, X_train_orig[:, :, :, 2], x, y)        \n",
    "test_data[:, :, 2] = scan_image_with_hilbert(N, mtest, X_test_orig[:, :, :, 2], x, y) \n",
    "\n",
    "# new shapes\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the model as a Sequential model in Keras. We define the model as having two blocks of two 1D CNN layers followed by  a pooling layer, then a dropout layer for regularization. After that, the learned features are flattened to a vector and pass through a fully connected layer. Then the output layer used to make predictions. We use batch normalization and ELU activation function elsewhere. Adam optimizer with default learning rate of 0.001 was used. We trained this model for 10 epochs using batches of 32 examples and achieved 88% train accuracy and 70.06% test accuracy. It took 1550 seconds to run. \n",
    "\n",
    "\n",
    "More details about the model hyperparameters are below:\n",
    "\n",
    "In the first block we use\n",
    "- 1D convolutional layers with 32 filters (feature detectors), each of length 4 (kernel size or sliding window). In this initial layer, 32 sliding windows of size 4 will run through the data to learn basic features.\n",
    "- The exponential linear unit (ELU) activation function has a nonzero gradient for $z < 0$. It was used to avoid the dying units issue (some neurons die and output 0) when using RELU.\n",
    "- To zero-center and normalize the inputs, we use the batch normalization techique that evaluates mean and standard deviation  of the inputs over the current mini-batch. This technique makes the networks much less sensitive to the weight initialization.\n",
    "- MaxPooling of size 2 and stride 1 was applied by taking the max value of every 2 features.\n",
    "- Dropout layer with rate 0.25 was used to ensure that at every trainig step, every input neuron has the probability 0.25 of being temporarily dropped (ignored).\n",
    "\n",
    "The second block is similar to the first one, except more filters were used and AveragePooling was used instead of MaxPooling. After that we 'flatten' and turn the output into a vector. The first fully connected layer defined as Dense(128) takes the input from the features analysis and applies weights to predict the correct label. The last output layer defined by Dense(num_classes) uses a softmax activation function to give probabilities for the num_classes=10 output classes.\n",
    "\n",
    "\n",
    "We train our model for 10 epochs, meaning that we completely pass through the training dataset 10 times. After evey 32 training examples (batch_size=32) the model's internal parameters were updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 154s 3ms/step - loss: 1.4306 - acc: 0.5003 - val_loss: 1.1097 - val_acc: 0.6051\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 153s 3ms/step - loss: 1.0699 - acc: 0.6208 - val_loss: 1.0559 - val_acc: 0.6265\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 152s 3ms/step - loss: 0.9221 - acc: 0.6740 - val_loss: 0.8827 - val_acc: 0.6910\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 153s 3ms/step - loss: 0.7974 - acc: 0.7192 - val_loss: 0.8785 - val_acc: 0.6874\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 155s 3ms/step - loss: 0.6768 - acc: 0.7604 - val_loss: 0.8992 - val_acc: 0.6869\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 151s 3ms/step - loss: 0.5796 - acc: 0.7943 - val_loss: 0.9118 - val_acc: 0.6985\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 160s 3ms/step - loss: 0.4925 - acc: 0.8261 - val_loss: 0.9535 - val_acc: 0.7019\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 158s 3ms/step - loss: 0.4293 - acc: 0.8461 - val_loss: 1.0369 - val_acc: 0.6846\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 154s 3ms/step - loss: 0.3873 - acc: 0.8627 - val_loss: 0.9445 - val_acc: 0.7053\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 151s 3ms/step - loss: 0.3403 - acc: 0.8800 - val_loss: 1.0641 - val_acc: 0.7006\n",
      "Test loss: 1.0641462841033935\n",
      "Test accuracy: 0.7006\n",
      "--- 1550.0947885513306 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(2802)\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "#сreating 32 different filters, each of them with length 4.\n",
    "model.add(Conv1D(32, 4, input_shape=train_data.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(32, 4))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(64, 4))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64, 4))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_data, Y_test))\n",
    "score = model.evaluate(test_data, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we read pixels with Hilbert curve. Here we read pixels with modified Sierpinski curve and use the same model as above to train 1D CNNs. After 10 epochs the model achieved 87.52% train accuracy and 70.37% test accuracy in 1546 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 156s 3ms/step - loss: 1.4329 - acc: 0.4987 - val_loss: 1.1518 - val_acc: 0.5845\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 154s 3ms/step - loss: 1.0670 - acc: 0.6209 - val_loss: 1.0010 - val_acc: 0.6444\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 154s 3ms/step - loss: 0.9283 - acc: 0.6723 - val_loss: 0.8897 - val_acc: 0.6869\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 153s 3ms/step - loss: 0.7928 - acc: 0.7203 - val_loss: 0.8774 - val_acc: 0.6927\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 157s 3ms/step - loss: 0.6923 - acc: 0.7549 - val_loss: 0.9136 - val_acc: 0.6952\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 156s 3ms/step - loss: 0.5892 - acc: 0.7941 - val_loss: 0.9107 - val_acc: 0.6870\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 154s 3ms/step - loss: 0.5135 - acc: 0.8183 - val_loss: 0.8971 - val_acc: 0.7048\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 154s 3ms/step - loss: 0.4404 - acc: 0.8443 - val_loss: 1.0022 - val_acc: 0.6881\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 150s 3ms/step - loss: 0.3957 - acc: 0.8574 - val_loss: 1.0112 - val_acc: 0.6935\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 153s 3ms/step - loss: 0.3514 - acc: 0.8752 - val_loss: 1.0020 - val_acc: 0.7037\n",
      "Test loss: 1.0020179424285889\n",
      "Test accuracy: 0.7037\n",
      "--- 1546.754551410675 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# read with modified Sierpinski curve\n",
    "\n",
    "x,y = modified_sierpinski_curve(N)\n",
    "\n",
    "train_data = np.zeros([X_train_orig.shape[0], 2**(2*N), X_train_orig.shape[3]])\n",
    "test_data = np.zeros([X_test_orig.shape[0], 2**(2*N), X_test_orig.shape[3]])\n",
    "\n",
    "train_data[:, :, 0] = scan_image_with_hilbert(N, mtrain, X_train_orig[:, :, :, 0], x, y)        \n",
    "test_data[:, :, 0] = scan_image_with_hilbert(N, mtest, X_test_orig[:, :, :, 0], x, y) \n",
    "\n",
    "train_data[:, :, 1] = scan_image_with_hilbert(N, mtrain, X_train_orig[:, :, :, 1], x, y)        \n",
    "test_data[:, :, 1] = scan_image_with_hilbert(N, mtest, X_test_orig[:, :, :, 1], x, y) \n",
    "\n",
    "train_data[:, :, 2] = scan_image_with_hilbert(N, mtrain, X_train_orig[:, :, :, 2], x, y)        \n",
    "test_data[:, :, 2] = scan_image_with_hilbert(N, mtest, X_test_orig[:, :, :, 2], x, y) \n",
    "\n",
    "\n",
    "\n",
    "# train 1D CNNs\n",
    "random.seed(2802)\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(32, 4, input_shape=train_data.shape[1:]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(32, 4))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv1D(64, 4))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(64, 4))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test_data, Y_test))\n",
    "score = model.evaluate(test_data, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply 2D CNNs to the original data. We use similar architecture as for 1D CNNs. The model achieves 77.44% train accuracy and 77.14% test accuracy in 1353 seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 133s 3ms/step - loss: 1.5394 - acc: 0.4585 - val_loss: 1.7792 - val_acc: 0.3948\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 132s 3ms/step - loss: 1.0983 - acc: 0.6124 - val_loss: 0.9536 - val_acc: 0.6668\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 134s 3ms/step - loss: 0.9421 - acc: 0.6671 - val_loss: 0.8806 - val_acc: 0.6908\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 135s 3ms/step - loss: 0.8605 - acc: 0.6993 - val_loss: 0.7699 - val_acc: 0.7276\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 136s 3ms/step - loss: 0.7957 - acc: 0.7208 - val_loss: 0.8048 - val_acc: 0.7161\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 135s 3ms/step - loss: 0.7545 - acc: 0.7366 - val_loss: 0.8438 - val_acc: 0.7029\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 135s 3ms/step - loss: 0.7180 - acc: 0.7496 - val_loss: 0.7697 - val_acc: 0.7301\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 135s 3ms/step - loss: 0.6870 - acc: 0.7629 - val_loss: 0.8578 - val_acc: 0.7087\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 137s 3ms/step - loss: 0.6633 - acc: 0.7678 - val_loss: 0.6602 - val_acc: 0.7688\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 136s 3ms/step - loss: 0.6401 - acc: 0.7744 - val_loss: 0.6594 - val_acc: 0.7714\n",
      "Test loss: 0.6594155699253083\n",
      "Test accuracy: 0.7714\n",
      "--- 1353.6397528648376 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "random.seed(2802)\n",
    "start_time = time.time()\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding='same',\n",
    "                 input_shape=X_train_orig.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(AveragePooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3,3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_orig, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_orig, Y_test))\n",
    "score = model.evaluate(X_test_orig, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#loss: 0.6401 - acc: 0.7744 - val_loss: 0.6594 - val_acc: 0.7714 1353.63 seconds ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "BFd89",
   "launcher_item_id": "AH2rK"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
